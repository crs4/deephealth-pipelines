version: "3"


x-aliases:  # if it starts with x- it's ignored by docker-compose
    - &airflow_env_var
        environment:
            - AIRFLOW_HOME=${AIRFLOW_HOME}
            - CWL_INPUTS_FOLDER=${CWL_INPUTS_FOLDER}
            - PROCESS_REPORT_URL=${PROCESS_REPORT_URL}
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql://${MYSQL_USER}:${MYSQL_PASSWORD}@mysql:3306/${MYSQL_DATABASE}
            - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_HOME}/dags
            - AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_HOME}/logs
            - AIRFLOW__CORE__DAG_PROCESSOR_MANAGER_LOG_LOCATION=${AIRFLOW_HOME}/logs/dag_processor_manager/dag_processor_manager.log
            - AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_HOME}/plugins
            - AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_HOME}/logs/scheduler
            - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
            - AIRFLOW__CWL__INPUTS_FOLDER=${CWL_INPUTS_FOLDER}
            - AIRFLOW_USER=${AIRFLOW_USER}
            - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
            - CWLDOCKER_GPUS=$CWLDOCKER_GPUS
            - INPUT_DIR=${INPUT_DIR}
            - FAILED_DIR=${FAILED_DIR}
            - BACKUP_DIR=${BACKUP_DIR}
            - OUT_DIR=${CWL_OUTPUTS_FOLDER}
            - OME_SEADRAGON_URL=${OME_SEADRAGON_URL}
            - PROMORT_HOST=${PROMORT_HOST}
            - PROMORT_CONN_TYPE=${PROMORT_CONN_TYPE}
            - PROMORT_PORT=${PROMORT_PORT}
            - PROMORT_USER=${PROMORT_USER}
            - PROMORT_PASSWORD=${PROMORT_PASSWORD}
            - PROMORT_SESSION_ID=${PROMORT_SESSION_ID}
            - PREDICTIONS_DIR=${PREDICTIONS_DIR}
            - DOCKER_NETWORK=${DOCKER_NETWORK}
            - PROMORT_TOOLS_IMG=${PROMORT_TOOLS_IMG}

    - &airflow_volumes
        volumes:
            - ${AIRFLOW_HOME}:${AIRFLOW_HOME}
            - ${CWL_TMP_FOLDER}:${CWL_TMP_FOLDER}
            - ${CWL_INPUTS_FOLDER}:${CWL_INPUTS_FOLDER}
            - ${CWL_OUTPUTS_FOLDER}:${CWL_OUTPUTS_FOLDER}
            - ${CWL_PICKLE_FOLDER}:${CWL_PICKLE_FOLDER}
    - &build
        build:
          context: ./build/cwl-airflow/packaging/docker_compose/local_executor/cwl_airflow
          args:
            CWL_AIRFLOW_VERSION: ${CWL_AIRFLOW_VERSION}
            CWL_AIRFLOW_URL: ${CWL_AIRFLOW_URL}
            EXTRA_PIP_DEPS: ${EXTRA_PIP_DEPS}

services:

    # starts after mysql was launched, then waits until airflow gets access to the database,
    # then runs cwl-airflow init creating all required tables in the database and starts scheduler
  scheduler:
      # image: mdrio/cwl-airflow-scheduler
      <<: *build
      container_name: scheduler
      volumes:  # can't reuse *airflow_volumes as YAML doesn't support sequence merging
          - /var/run/docker.sock:/var/run/docker.sock
          - ${AIRFLOW_HOME}:${AIRFLOW_HOME}
          - ${CWL_TMP_FOLDER}:${CWL_TMP_FOLDER}
          - ${CWL_INPUTS_FOLDER}:${CWL_INPUTS_FOLDER}
          - ${CWL_OUTPUTS_FOLDER}:${CWL_OUTPUTS_FOLDER}
          - ${CWL_PICKLE_FOLDER}:${CWL_PICKLE_FOLDER}
          - ./cwl/:/cwl
          - ${PREDICTIONS_DIR}:${PREDICTIONS_DIR}
          - ${INPUT_DIR}:${INPUT_DIR}
          - ${FAILED_DIR}:${FAILED_DIR}
          - ${BACKUP_DIR}:${BACKUP_DIR}

      # privileged: true                                               # maybe I don't actully need it here
      restart: always
      <<: *airflow_env_var
      # <<: *airflow_env_file
      command: start_scheduler.sh
      depends_on:
          - mysql
      networks:
        deephealth:

  # starts after scheduler was launched, then waits untill all required for airflow tables have been
  # created (we just check dag_run table) and starts webserver
  webserver:
      # image: mdrio/cwl-airflow-webserver
      <<: *build
      container_name: webserver
      ports:
          - ${AIRFLOW_WEBSERVER_PORT}:8080
      <<: *airflow_volumes
      <<: *airflow_env_var
      # <<: *airflow_env_file
      restart: always
      command: start_webserver.sh
      depends_on:
          - scheduler  # need to start only after cwl-airflow init was run
      networks:
        deephealth:

  # starts after scheduler was launched, then waits untill all required for airflow tables have been
  # created (we just check dag_run table) and starts apiserver
  # the default API --host 127.0.0.1 won't work from inside the docker container, so we used 0.0.0.0
  apiserver:
      # image: mdrio/cwl-airflow-apiserver
      <<: *build
      container_name: apiserver
      ports:
          - ${CWL_AIRFLOW_API_PORT}:8081
      <<: *airflow_volumes        
      <<: *airflow_env_var
      # <<: *airflow_env_file
      restart: always
      command: start_apiserver.sh --replay 60 --host 0.0.0.0
      depends_on:
          - scheduler  # need to start only after cwl-airflow init was run
      networks:
        deephealth:

  # starts MySQL server, creates empty ${MYSQL_DATABASE} with ${MYSQL_USER} and ${MYSQL_PASSWORD}
  mysql:
      image: mysql:5.7
      ports:
          - ${MYSQL_PORT}:3306
      volumes:
          - ${MYSQL_DATA}:/var/lib/mysql
          - /dev/urandom:/dev/random  # Not sure if I need it at all
      restart: always
      environment:
          - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
          - MYSQL_DATABASE=${MYSQL_DATABASE}
          - MYSQL_USER=${MYSQL_USER}
          - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      command: --explicit-defaults-for-timestamp=1
      networks:
        deephealth:


  init:
      <<: *build
      container_name: init
      volumes:  # can't reuse *airflow_volumes as YAML doesn't support sequence merging
        - ${AIRFLOW_HOME}:${AIRFLOW_HOME}
        - ./scripts/:/scripts
      <<: *airflow_env_var
      # <<: *airflow_env_file
      restart: on-failure
      entrypoint: bash
      command: [ "/scripts/init.sh"]
      depends_on:
          - scheduler  # need to start only after cwl-airflow init was run
          - mysql
      networks:
        deephealth:

networks:
  deephealth:
    name: ${DOCKER_NETWORK}


